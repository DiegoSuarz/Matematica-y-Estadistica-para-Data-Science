{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmgrHTazuap0"
      },
      "source": [
        "# Programando Red Neuronal desde Cero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dqFjSUKuaqk"
      },
      "source": [
        "El objetivo de este ejercicio es clasificar un una clase , un dataset de puntos distribuidos de forma circular de otro dataset de puntos. Este problema de clasificación solo puede ser resuelto con una curva cerrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFE6QWLFuaqp"
      },
      "outputs": [],
      "source": [
        "#Librerias necesarias para este proyecto\n",
        "import numpy as np\n",
        "import scipy as sc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#La librería SK Learn es requerida para crear los datasets.\n",
        "from sklearn.datasets import make_circles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnJg-p92uaqy"
      },
      "outputs": [],
      "source": [
        "# make_circles?\n",
        "#Amable recordatorio: con ? puedes acceder a la documentación del método"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGOIrd0guaq2"
      },
      "outputs": [],
      "source": [
        "#Inicializar las variables n y p.\n",
        "#Donde n es el tamaño dl dataset y p es el número de atributos que contiene un punto.\n",
        "n = 500 #El número de registros\n",
        "p = 2 #¿Cuantas características tengo en cada dato?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtYv0sAirbaA"
      },
      "source": [
        "## 1. Definimos el dataset que queremos clasificar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_cyttlCiB4k"
      },
      "source": [
        "A continuación, vamos a inicializar el dataset de puntos que vamos a clasificar. La variable X son las tuplas que contienen las coordenadas en el plano de cada punto, la variable Y es un vector que define a que clase (dos clases, 0 o 1) pertenece cada punto.**bold text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj81KRsDuaq8"
      },
      "outputs": [],
      "source": [
        "X, Y = make_circles(n_samples=n, factor=0.5, noise = 0.05)\n",
        "#el parámetro factor separa las dispersiones de puntos.\n",
        "#el parámetro noise es el indice de disperción de los puntos en el plano.\n",
        "\n",
        "Y = Y[:, np.newaxis] #Esta linea cambia la estructura del vector de salida.\n",
        "print(Y.shape)\n",
        "\n",
        "plt.scatter(X[:, 0],X[:, 1] ) #La imagen muestra las dispersiones de ambas clases.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn4eaT38uarW"
      },
      "source": [
        "A continuación, vamos a visualizar los puntos, representando las clases en dos colores (0 para la clase \"blue\", 1 para la clase \"red\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3SbtmY6uarY",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plt.scatter(X[Y[:, 0] == 0, 0],X[Y[:, 0] == 0, 1], c=\"blue\" )\n",
        "plt.scatter(X[Y[:, 0] == 1, 0],X[Y[:, 0] == 1, 1], c=\"red\" )\n",
        "plt.axis(\"equal\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtAwgmsGuarb"
      },
      "source": [
        "A continuación, crearemos una clase (en el contexto del paradigma de POO) que contenga los atributos de una capa de neuronas. Esta estructura contiene el número de conexiones, el número de neuronas, la función de activación, el vector de sesgos y la matriz de pesos. Tanto el vector de sesgos como la matriz de pesos contienen valores al azar normalizados (valores aleatorios entre -1 y 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh9QaOjKkg-9"
      },
      "source": [
        "Pregunta 1: Entiendo porque los datos del vector de sesgos y de la matriz de pesos deben ser aleatorios, pero ¿Por qué deben ser normalizados y en un rango entre -1 y 1?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTmjRx05sFpY"
      },
      "source": [
        "##2. Definimos la estructura de la red neuronal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBZO4NLzuare"
      },
      "outputs": [],
      "source": [
        "#Clase de la capa de la red\n",
        "\n",
        "class neural_layer():\n",
        "  def __init__(self, n_conn, n_neur, act_f):\n",
        "    self.act_f = act_f\n",
        "\n",
        "    self.b = np.random.rand(1, n_neur)*2-1\n",
        "    self.W = np.random.rand(n_conn, n_neur)*2-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d915UIdtuarg"
      },
      "source": [
        "En la siguiente celda definiremos la función de activación de la tangente hiperbólica y su derivada. Ambas funciones están en una tupla, donde la primer componente de la tupla es la función y la segunda componente es su derivada.\n",
        "\n",
        "$$\\phi (x)= tanh (x) = \\frac{e^{x}-e^{-x}}{e^{-x}+e^{x}}$$\n",
        "\n",
        "$$\\phi' (x)= sech^{2}(x) = \\frac{4}{(e^{-x}+e^{x})^{2}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heCdw97duari"
      },
      "outputs": [],
      "source": [
        "#Funciones de activación\n",
        "\n",
        "#NOTA 1: Las simulaciones en el playground de Tensorflow  demuestran que funciona mejor\n",
        "# con la función de tanh.\n",
        "\n",
        "tanh = (lambda x : ((np.e**x)-(np.e**(-x)))/((np.e**x)+(np.e**(-x))),\n",
        "        lambda x : 4/((((np.e**x)+(np.e**(-x))))**2))\n",
        "\n",
        "_x = np.linspace(-5,5, 100)\n",
        "plt.plot(_x, tanh[1](_x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSJwtJyRuarm"
      },
      "outputs": [],
      "source": [
        "#Esta función ReLu es solo demostrativa, no se utilizará en el proyecto\n",
        "\n",
        "ReLu = lambda x : np.maximum(0,x)\n",
        "_x = np.linspace(-5,5, 100)\n",
        "plt.plot(_x, ReLu(_x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIYDOAvtmMOE"
      },
      "source": [
        "Es necesario notar, que la estructura ```neural_layer``` requiere de tres argumentos (número de conexiones a la entrada, número de neuronas en la capa, función de activación característica de la capa). Con la ayuda del vector ```topology```, característico de la forma de toda la red, podremos automatizar la construcción de toda la red."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1SGPTVhuarx"
      },
      "outputs": [],
      "source": [
        "l0 = neural_layer(n_conn = p, n_neur = 4 , act_f = tanh) #capa de entrada\n",
        "l1 = neural_layer(n_conn = 4, n_neur = 8 , act_f = tanh) #1er capa oculta\n",
        "#···\n",
        "topology = [p, 4, 8, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l597jKIXxDhy"
      },
      "source": [
        "La función ```create_nn``` requiere dos argumentos:\n",
        "\n",
        "1.   El vector ```topology``` característico de la red, cada componente representa el número de neuronas por capa.\n",
        "\n",
        "2.   La función de activación ```act_f``` , que para este proyecto será la misma en todas las capas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhYgAZMtuarz"
      },
      "outputs": [],
      "source": [
        "def create_nn(topology, act_f):\n",
        "  nn = []\n",
        "\n",
        "  for l, layer in enumerate(topology[:-1]):\n",
        "    nn.append(neural_layer(topology[l], topology[l+1] , act_f))\n",
        "\n",
        "  return nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQksU821uasA",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "create_nn(topology, tanh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCMIz7XduasF"
      },
      "source": [
        "## 3. Definimos la función de entrenamiento de la red"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcXQsA3EzEs9"
      },
      "source": [
        "En la siguiente celda se puede observar, que la variable ```l2_cost``` se compone de una tupla de dos funciones lambda (sumatorias), donde la primera componente es la función de coste y la segunda componente es la derivada de la función de coste.\n",
        "\n",
        "La función de coste se define de la siguiente manera (Error cuadrático medio):\n",
        "\n",
        "$$MSE = \\frac{1}{n}\\sum_{i = 1}^{n}\\left ( Y_{i} - \\hat{Y}_{i} \\right )^{2}$$\n",
        "\n",
        "Pregunta: ¿Cómo se expresa la derivada de la función de coste?\n",
        "Fuente: https://sebastianraschka.com/faq/docs/mse-derivative.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1-0l2OrlQ2d"
      },
      "outputs": [],
      "source": [
        "neural_net = create_nn(topology, tanh)\n",
        "l2_cost = (lambda Yp, Yr: np.mean((Yp - Yr)**2), lambda Yp, Yr: 2*(Yp - Yr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLPfuDcIp5-l"
      },
      "source": [
        "En este punto del proyecto la red neuronal ya está construida, pero no ha sido entrenada ni ajustada.\n",
        "\n",
        "La función de entrenamiento contempla los siguientes pasos:\n",
        "\n",
        " 1. Propagación hacia adelante.\n",
        " 2. Comparar resultados obtenidos con los esperados.\n",
        " 3. Propagación hacia atras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F7R_VQXvvHk"
      },
      "outputs": [],
      "source": [
        "#Función de entrenamiento\n",
        "def train(neural_net, X, Y, f_cost,lr=0.5,train=True):\n",
        "  out=[(None,X)] # En este array guarda los pares de z y a [(z0,a0),(z1,a1),...]\n",
        "\n",
        "  #1. Propagación hacia adelante.\n",
        "  for l, layer in enumerate(neural_net):\n",
        "\n",
        "    z = np.dot(out[-1][1],neural_net[l].W) + neural_net[l].b\n",
        "    a = neural_net[l].act_f[0](z)\n",
        "\n",
        "    out.append((z,a))\n",
        "\n",
        "  #2. Backpropagation and Gradient descent\n",
        "\n",
        "  if train:\n",
        "    delta=[]\n",
        "\n",
        "    for l in reversed(range(0,len(neural_net))):\n",
        "      z=out[l+1][0]\n",
        "      a=out[l+1][1]\n",
        "\n",
        "      #Si estamos en la ultima capa\n",
        "      if l == len(neural_net) - 1:\n",
        "        delta.insert(0, f_cost[1](a, Y) * neural_net[l].act_f[1](a))\n",
        "      #Calculamos delta en las capas previas\n",
        "      else:\n",
        "        delta.insert(0,np.dot(delta[0],aux_w.T) * neural_net[l].act_f[1](a))\n",
        "\n",
        "      aux_w=neural_net[l].W\n",
        "      #Lo guardamos en una variable auxiliar para poder modificar los valores al mismo tiempo usando el Gradiente descendente\n",
        "\n",
        "      #Gradient Descent : Ajuste de la matriz de pesos y el valor del vector sesgo.\n",
        "      neural_net[l].b = neural_net[l].b - lr * np.mean(delta[0], axis=0, keepdims=True)\n",
        "      neural_net[l].W = neural_net[l].W - np.dot(out[l][1].T,delta[0])*lr\n",
        "\n",
        "  return out[-1][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J_QB0gQol0e"
      },
      "source": [
        "## 4. Enfrentamos a la red con los datos y presentamos resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMvG-L7OuasU"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "neural_n = create_nn(topology, tanh)\n",
        "loss = []\n",
        "\n",
        "for i in range(1000):\n",
        "\n",
        "  #Entrenamos la red:\n",
        "  pY = train(neural_n, X, Y, l2_cost, lr = 0.001)\n",
        "\n",
        "  if i%25 == 0:\n",
        "\n",
        "    loss.append(l2_cost[0](pY,Y))\n",
        "    res = 50\n",
        "    _x0 = np.linspace(-1.5, 1.5, res)\n",
        "    _x1 = np.linspace(-1.5, 1.5, res)\n",
        "\n",
        "    _Y = np.zeros((res, res))\n",
        "\n",
        "    for i0, x0 in enumerate(_x0):\n",
        "      for i1, x1 in enumerate(_x1):\n",
        "        _Y[i0, i1] = train(neural_n, np.array([[x0,x1]]), Y, l2_cost,train=False)[0][0]\n",
        "\n",
        "    #Visualizamos los resultados del entrenamiento.\n",
        "\n",
        "    plt.pcolormesh(_x0, _x1, _Y, cmap = \"coolwarm\")\n",
        "    plt.axis(\"equal\")\n",
        "\n",
        "    plt.scatter(X[Y[:,0] == 0, 0], X[Y[:,0] == 0, 1], c = \"skyblue\" )\n",
        "    plt.scatter(X[Y[:,0] == 1, 0], X[Y[:,0] == 1, 1], c = \"salmon\" )\n",
        "\n",
        "    clear_output(wait = True)\n",
        "    plt.show()\n",
        "    plt.plot(range(len(loss)), loss)\n",
        "    plt.show()\n",
        "    time.sleep(0.5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_xKidaifs3nx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}