{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img width=\"300px\" src=\"https://cachimbo.pe/wp-content/uploads/2022/10/1-19.jpg\"></img>\n",
        "\n",
        "#**Matemática y Estadística para Ciencia de Datos**\n",
        "## **Tema: PCA y LDA**\n",
        "#### **Docente: Giron Rene Omar A.**\n",
        "\n",
        "---------------"
      ],
      "metadata": {
        "id": "gIEzjl6hT017"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA"
      ],
      "metadata": {
        "id": "Gn4RydTUQrvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pasos\n",
        "1. Centrar los datos\n",
        "2. Calcular la matriz de covarianza\n",
        "3. Encontrar autovalores y autovectores\n",
        "4. Ordenar autovalores y seleccionar autovectores principales\n",
        "5. Proyectar los datos en los nuevos ejes"
      ],
      "metadata": {
        "id": "aIfZVbSuI2MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Generar un conjunto de datos aleatorio\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 3)\n",
        "\n",
        "# Paso 1: Centrar los datos\n",
        "X_mean = np.mean(X, axis=0)\n",
        "X_centered = X - X_mean\n",
        "\n",
        "# Paso 2: Calcular la matriz de covarianza\n",
        "cov_matrix = np.cov(X_centered, rowvar=False)\n",
        "\n",
        "# Paso 3: Encontrar autovalores y autovectores\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "# Paso 4: Ordenar autovalores y seleccionar autovectores principales\n",
        "sorted_index = np.argsort(eigenvalues)[::-1]\n",
        "sorted_eigenvalues = eigenvalues[sorted_index]\n",
        "sorted_eigenvectors = eigenvectors[:, sorted_index]\n",
        "\n",
        "# Seleccionar los primeros k autovectores (por ejemplo, k=2)\n",
        "k = 2\n",
        "eigenvector_subset = sorted_eigenvectors[:, 0:k]\n",
        "\n",
        "# Paso 5: Proyectar los datos en los nuevos ejes\n",
        "X_reduced = np.dot(X_centered, eigenvector_subset)\n",
        "\n",
        "# Gráfico de los datos originales en 3D\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "ax1 = fig.add_subplot(121, projection='3d')\n",
        "ax1.scatter(X_centered[:, 0], X_centered[:, 1], X_centered[:, 2], edgecolor='k', s=50, alpha=0.7)\n",
        "ax1.set_xlabel('X1')\n",
        "ax1.set_ylabel('X2')\n",
        "ax1.set_zlabel('X3')\n",
        "ax1.set_title('Datos Originales Centrados')\n",
        "\n",
        "# Gráfico de los datos proyectados en 2D\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax2.scatter(X_reduced[:, 0], X_reduced[:, 1], edgecolor='k', s=50, alpha=0.7)\n",
        "ax2.set_xlabel('Primer Componente Principal')\n",
        "ax2.set_ylabel('Segundo Componente Principal')\n",
        "ax2.set_title('Datos Proyectados en los Primeros Dos Componentes Principales')\n",
        "ax2.grid()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(\"Autovalores:\")\n",
        "print(sorted_eigenvalues)\n",
        "\n",
        "print(\"\\nAutovectores:\")\n",
        "print(sorted_eigenvectors)\n",
        "\n",
        "print(\"\\nDatos originales centrados:\")\n",
        "print(X_centered[:5])  # Muestra las primeras 5 filas\n",
        "\n",
        "print(\"\\nDatos proyectados (primeras 5 filas):\")\n",
        "print(X_reduced[:5])"
      ],
      "metadata": {
        "id": "0zE23zBuI8-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "\n",
        "# Generar un conjunto de datos aleatorio\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 3)\n",
        "\n",
        "# Paso 1: Centrar los datos\n",
        "X_mean = np.mean(X, axis=0)\n",
        "X_centered = X - X_mean\n",
        "\n",
        "# Paso 2: Calcular la matriz de covarianza\n",
        "cov_matrix = np.cov(X_centered, rowvar=False)\n",
        "\n",
        "# Paso 3: Encontrar autovalores y autovectores\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "# Paso 4: Ordenar autovalores y seleccionar autovectores principales\n",
        "sorted_index = np.argsort(eigenvalues)[::-1]\n",
        "sorted_eigenvalues = eigenvalues[sorted_index]\n",
        "sorted_eigenvectors = eigenvectors[:, sorted_index]\n",
        "\n",
        "# Paso 5: Proyectar los datos en los nuevos ejes\n",
        "X_reduced = np.dot(X_centered, sorted_eigenvectors)\n",
        "\n",
        "# Graficar el gráfico de codo\n",
        "explained_variance_ratio = sorted_eigenvalues / np.sum(sorted_eigenvalues)\n",
        "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
        "plt.xlabel('Número de Componentes Principales')\n",
        "plt.ylabel('Varianza Explicada Acumulativa')\n",
        "plt.title('Gráfico de Codo')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Aplicar el criterio de Kaiser\n",
        "kaiser_criterion = np.sum(sorted_eigenvalues > 1)\n",
        "print(f'Número de componentes principales según el criterio de Kaiser: {kaiser_criterion}')\n",
        "\n",
        "# Gráfico de varianza explicada\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.6, align='center',\n",
        "        label='Varianza Explicada Individual')\n",
        "plt.step(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, where='mid', linestyle='--',\n",
        "         label='Varianza Explicada Acumulativa')\n",
        "plt.xlabel('Número de Componentes Principales')\n",
        "plt.ylabel('Varianza Explicada')\n",
        "plt.title('Gráfico de Varianza Explicada')\n",
        "plt.legend(loc='best')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Calcular las cargas factoriales\n",
        "factor_loadings = sorted_eigenvectors * np.sqrt(sorted_eigenvalues)\n",
        "\n",
        "# Biplot\n",
        "def biplot(score, coeff, y, labels=None):\n",
        "    xs = score[:, 0]\n",
        "    ys = score[:, 1]\n",
        "    n = coeff.shape[0]\n",
        "    scalex = 1.0 / (xs.max() - xs.min())\n",
        "    scaley = 1.0 / (ys.max() - ys.min())\n",
        "    plt.scatter(xs * scalex, ys * scaley, c=y)\n",
        "    for i in range(n):\n",
        "        plt.arrow(0, 0, coeff[i, 0], coeff[i, 1], color='r', alpha=0.5)\n",
        "        if labels is None:\n",
        "            plt.text(coeff[i, 0] * 1.15, coeff[i, 1] * 1.15, \"Var\"+str(i+1), color='g', ha='center', va='center')\n",
        "        else:\n",
        "            plt.text(coeff[i, 0] * 1.15, coeff[i, 1] * 1.15, labels[i], color='g', ha='center', va='center')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "biplot(X_reduced[:, :2], factor_loadings[:, :2], y=np.zeros(X_reduced.shape[0]), labels=['Var1', 'Var2', 'Var3'])\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(\"Autovalores:\")\n",
        "print(sorted_eigenvalues)\n",
        "\n",
        "print(\"\\nAutovectores:\")\n",
        "print(sorted_eigenvectors)\n",
        "\n",
        "print(\"\\nCargas Factoriales:\")\n",
        "print(factor_loadings)\n",
        "\n",
        "print(\"\\nDatos proyectados (primeras 5 filas):\")\n",
        "print(X_reduced[:5])\n"
      ],
      "metadata": {
        "id": "FGkeFiPnuG7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA"
      ],
      "metadata": {
        "id": "sLFDzV_GQb6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caso 1"
      ],
      "metadata": {
        "id": "wckTBPHuSZzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generar un conjunto de datos simple\n",
        "np.random.seed(42)\n",
        "X_class1 = np.random.randn(50, 2) + np.array([2, 2])\n",
        "X_class2 = np.random.randn(50, 2) + np.array([-2, -2])\n",
        "X = np.vstack((X_class1, X_class2))\n",
        "y = np.array([0]*50 + [1]*50)\n",
        "\n",
        "# Paso 1: Calcular la media de cada clase y la media global\n",
        "mean_class1 = np.mean(X_class1, axis=0)\n",
        "mean_class2 = np.mean(X_class2, axis=0)\n",
        "mean_global = np.mean(X, axis=0)\n",
        "\n",
        "# Paso 2: Calcular las matrices de dispersión intra-clase (S_W) y entre-clase (S_B)\n",
        "S_W = np.zeros((2, 2))\n",
        "for x in X_class1:\n",
        "    S_W += (x - mean_class1).reshape(2, 1).dot((x - mean_class1).reshape(1, 2))\n",
        "for x in X_class2:\n",
        "    S_W += (x - mean_class2).reshape(2, 1).dot((x - mean_class2).reshape(1, 2))\n",
        "\n",
        "S_B = (mean_class1 - mean_global).reshape(2, 1).dot((mean_class1 - mean_global).reshape(1, 2)) * 50\n",
        "S_B += (mean_class2 - mean_global).reshape(2, 1).dot((mean_class2 - mean_global).reshape(1, 2)) * 50\n",
        "\n",
        "# Paso 3: Calcular los autovectores y autovalores de S_W^{-1}S_B\n",
        "eigenvalues, eigenvectors = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
        "\n",
        "# Paso 4: Selección de los autovectores correspondientes a los mayores autovalores\n",
        "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
        "W = sorted_eigenvectors[:, :1]  # Seleccionamos el primer autovector\n",
        "\n",
        "# Paso 5: Proyectar los datos en el espacio de los autovectores seleccionados\n",
        "X_lda = X.dot(W)\n",
        "\n",
        "# Visualización de los datos proyectados\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Datos originales\n",
        "plt.subplot(121)\n",
        "plt.scatter(X_class1[:, 0], X_class1[:, 1], label='Clase 1')\n",
        "plt.scatter(X_class2[:, 0], X_class2[:, 1], label='Clase 2')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "plt.title('Datos Originales')\n",
        "plt.legend()\n",
        "\n",
        "# Datos proyectados\n",
        "plt.subplot(122)\n",
        "plt.scatter(X_lda[y==0], np.zeros(50), label='Clase 1')\n",
        "plt.scatter(X_lda[y==1], np.zeros(50), label='Clase 2')\n",
        "plt.xlabel('Componente LDA 1')\n",
        "plt.title('Datos Proyectados en el Espacio LDA')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(\"Autovalores:\")\n",
        "print(eigenvalues)\n",
        "\n",
        "print(\"\\nAutovectores:\")\n",
        "print(eigenvectors)\n",
        "\n",
        "print(\"\\nDatos proyectados (primeras 5 filas):\")\n",
        "print(X_lda[:5])\n"
      ],
      "metadata": {
        "id": "JhHynd4eM_9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caso 2"
      ],
      "metadata": {
        "id": "Rgf6bG91SdZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Generar un conjunto de datos aleatorio con tres dimensiones\n",
        "np.random.seed(42)\n",
        "X_class1 = np.random.randn(50, 3) + np.array([3, 3, 3])\n",
        "X_class2 = np.random.randn(50, 3) + np.array([-3, -3, -3])\n",
        "X_class3 = np.random.randn(50, 3) + np.array([3, -3, 0])\n",
        "X = np.vstack((X_class1, X_class2, X_class3))\n",
        "y = np.array([0]*50 + [1]*50 + [2]*50)\n",
        "\n",
        "# Paso 1: Calcular la media de cada clase y la media global\n",
        "mean_class1 = np.mean(X_class1, axis=0)\n",
        "mean_class2 = np.mean(X_class2, axis=0)\n",
        "mean_class3 = np.mean(X_class3, axis=0)\n",
        "mean_global = np.mean(X, axis=0)\n",
        "\n",
        "# Paso 2: Calcular las matrices de dispersión intra-clase (S_W) y entre-clase (S_B)\n",
        "S_W = np.zeros((3, 3))\n",
        "for x in X_class1:\n",
        "    S_W += (x - mean_class1).reshape(3, 1).dot((x - mean_class1).reshape(1, 3))\n",
        "for x in X_class2:\n",
        "    S_W += (x - mean_class2).reshape(3, 1).dot((x - mean_class2).reshape(1, 3))\n",
        "for x in X_class3:\n",
        "    S_W += (x - mean_class3).reshape(3, 1).dot((x - mean_class3).reshape(1, 3))\n",
        "\n",
        "S_B = 50 * (mean_class1 - mean_global).reshape(3, 1).dot((mean_class1 - mean_global).reshape(1, 3))\n",
        "S_B += 50 * (mean_class2 - mean_global).reshape(3, 1).dot((mean_class2 - mean_global).reshape(1, 3))\n",
        "S_B += 50 * (mean_class3 - mean_global).reshape(3, 1).dot((mean_class3 - mean_global).reshape(1, 3))\n",
        "\n",
        "# Paso 3: Calcular los autovectores y autovalores de S_W^{-1}S_B\n",
        "eigenvalues, eigenvectors = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
        "\n",
        "# Paso 4: Selección de los autovectores correspondientes a los mayores autovalores\n",
        "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
        "W = sorted_eigenvectors[:, :2]  # Seleccionamos los dos primeros autovectores\n",
        "\n",
        "# Paso 5: Proyectar los datos en el espacio de los autovectores seleccionados\n",
        "X_lda = X.dot(W)\n",
        "\n",
        "# Visualización de los datos proyectados\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Datos originales en 3D\n",
        "ax1 = plt.subplot(121, projection='3d')\n",
        "ax1.scatter(X_class1[:, 0], X_class1[:, 1], X_class1[:, 2], label='Clase 1', alpha=0.7)\n",
        "ax1.scatter(X_class2[:, 0], X_class2[:, 1], X_class2[:, 2], label='Clase 2', alpha=0.7)\n",
        "ax1.scatter(X_class3[:, 0], X_class3[:, 1], X_class3[:, 2], label='Clase 3', alpha=0.7)\n",
        "ax1.set_xlabel('X1')\n",
        "ax1.set_ylabel('X2')\n",
        "ax1.set_zlabel('X3')\n",
        "ax1.set_title('Datos Originales en 3D')\n",
        "ax1.legend()\n",
        "\n",
        "# Datos proyectados en 2D\n",
        "ax2 = plt.subplot(122)\n",
        "ax2.scatter(X_lda[y==0, 0], X_lda[y==0, 1], label='Clase 1', alpha=0.7)\n",
        "ax2.scatter(X_lda[y==1, 0], X_lda[y==1, 1], label='Clase 2', alpha=0.7)\n",
        "ax2.scatter(X_lda[y==2, 0], X_lda[y==2, 1], label='Clase 3', alpha=0.7)\n",
        "ax2.set_xlabel('Componente LDA 1')\n",
        "ax2.set_ylabel('Componente LDA 2')\n",
        "ax2.set_title('Datos Proyectados en el Espacio LDA')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(\"Autovalores:\")\n",
        "print(eigenvalues)\n",
        "\n",
        "print(\"\\nAutovectores:\")\n",
        "print(eigenvectors)\n",
        "\n",
        "print(\"\\nDatos proyectados (primeras 5 filas):\")\n",
        "print(X_lda[:5])\n"
      ],
      "metadata": {
        "id": "LyUddwTcM_6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caso 3"
      ],
      "metadata": {
        "id": "1dUKyETVSgu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import library\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "feature_dict = {i:label for i,label in zip(\n",
        "                range(4),\n",
        "                  ('sepal length in cm',\n",
        "                  'sepal width in cm',\n",
        "                  'petal length in cm',\n",
        "                  'petal width in cm', ))}\n",
        "\n",
        "\n",
        "# Reading in the dataset\n",
        "\n",
        "df = pd.io.parsers.read_csv(\n",
        "    filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
        "    header=None,\n",
        "    sep=',',\n",
        "    )\n",
        "df.columns = [l for i,l in sorted(feature_dict.items())] + ['class label']\n",
        "df.dropna(how=\"all\", inplace=True) # to drop the empty line at file-end\n",
        "\n",
        "\n",
        "# use the LabelEncode from the scikit-learn library to convert the class labels into numbers: 1, 2, and 3\n",
        "\n",
        "X = df.iloc[:,[0,1,2,3]].values\n",
        "y = df['class label'].values\n",
        "\n",
        "enc = LabelEncoder()\n",
        "label_encoder = enc.fit(y)\n",
        "y = label_encoder.transform(y) + 1\n",
        "\n",
        "label_dict = {1: 'Setosa', 2: 'Versicolor', 3:'Virginica'}\n",
        "\n",
        "# LDA\n",
        "sklearn_lda = LDA(n_components=2)\n",
        "X_lda_sklearn = sklearn_lda.fit_transform(X, y)\n",
        "\n",
        "def plot_scikit_lda(X, title):\n",
        "\n",
        "    ax = plt.subplot(111)\n",
        "    for label,marker,color in zip(\n",
        "        range(1,4),('^', 's', 'o'),('blue', 'red', 'green')):\n",
        "\n",
        "        plt.scatter(x=X[:,0][y == label],\n",
        "                    y=X[:,1][y == label] * -1, # flip the figure\n",
        "                    marker=marker,\n",
        "                    color=color,\n",
        "                    alpha=0.5,\n",
        "                    label=label_dict[label])\n",
        "\n",
        "    plt.xlabel('LD1')\n",
        "    plt.ylabel('LD2')\n",
        "\n",
        "    leg = plt.legend(loc='upper right', fancybox=True)\n",
        "    leg.get_frame().set_alpha(0.5)\n",
        "    plt.title(title)\n",
        "\n",
        "    # hide axis ticks\n",
        "    plt.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",\n",
        "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
        "\n",
        "    # remove axis spines\n",
        "    ax.spines[\"top\"].set_visible(False)\n",
        "    ax.spines[\"right\"].set_visible(False)\n",
        "    ax.spines[\"bottom\"].set_visible(False)\n",
        "    ax.spines[\"left\"].set_visible(False)\n",
        "\n",
        "    plt.grid()\n",
        "    plt.tight_layout\n",
        "    plt.show()\n",
        "\n",
        "plot_scikit_lda(X_lda_sklearn, title='Default LDA via scikit-learn')"
      ],
      "metadata": {
        "id": "ViD1pjO7PENA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sklearn_lda = LDA(n_components=2)\n",
        "ml_lda = sklearn_lda.fit(X, y)"
      ],
      "metadata": {
        "id": "8wosGsvQ-J7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ml_lda"
      ],
      "metadata": {
        "id": "l20qgarRlWd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = X[:5,:]\n",
        "X_new"
      ],
      "metadata": {
        "id": "K3i4Y_qelqdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ml_lda.predict(X_new)"
      ],
      "metadata": {
        "id": "VCCqVufcljcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ml_lda.predict_proba(X_new)"
      ],
      "metadata": {
        "id": "4e2SMhSKmd09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "y_pred = ml_lda.predict(X)\n",
        "print(accuracy_score(y, y_pred))\n",
        "print(confusion_matrix(y, y_pred))"
      ],
      "metadata": {
        "id": "-S1Mb9wXl9Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y, y_pred))"
      ],
      "metadata": {
        "id": "pGCM6DwimQu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VDhNmqermV2s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}