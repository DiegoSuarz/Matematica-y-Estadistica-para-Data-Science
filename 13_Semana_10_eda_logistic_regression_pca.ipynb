{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfTdqu9i9lJ-"
      },
      "source": [
        "# **EDA + Logistic Regression + PCA**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVeZYhbD9lKA"
      },
      "source": [
        "## Tabla de contenido\n",
        "\n",
        "El contenido de este núcleo se divide en varios temas que son los siguientes:\n",
        "\n",
        "- La maldición de la dimensionalidad\n",
        "- Introducción al Análisis de Componentes Principales\n",
        "- Importar bibliotecas de Python\n",
        "- Importar conjunto de datos\n",
        "-\tAnálisis exploratorio de datos\n",
        "- Dividir datos en conjunto de entrenamiento y prueba.\n",
        "- Ingeniería de características\n",
        "- Escalado de funciones\n",
        "- Modelo de regresión logística con todas las características.\n",
        "- Regresión logística con PCA\n",
        "- Seleccione el número correcto de dimensiones\n",
        "- Trazar la relación de varianza explicada con el número de dimensiones.\n",
        "-\tConclusión\n",
        "- Referencias\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-pkmhYm9lKA"
      },
      "source": [
        "## La maldición de la dimensionalidad\n",
        "\n",
        "Generalmente, los conjuntos de datos del mundo real contienen miles o millones de funciones para entrenar. Esta es una tarea que requiere mucho tiempo ya que hace que el entrenamiento sea extremadamente lento. En estos casos es muy difícil encontrar una buena solución. Este problema a menudo se conoce como la maldición de la dimensionalidad.\n",
        "\n",
        "\n",
        "**La maldición de la dimensionalidad** se refiere a varios fenómenos que surgen cuando analizamos y organizamos datos en espacios de alta dimensión (a menudo con cientos o miles de dimensiones) que no ocurren en entornos de baja dimensión. El problema es que cuando aumenta la dimensionalidad, el volumen del espacio aumenta tan rápido que los datos disponibles se vuelven escasos. Esta escasez es problemática para cualquier método que requiera significación estadística.\n",
        "\n",
        "\n",
        "En problemas del mundo real, a menudo es posible reducir considerablemente el número de dimensiones. Este proceso se llama **reducción de dimensionalidad**. Se refiere al proceso de reducir el número de dimensiones consideradas mediante la obtención de un conjunto de variables principales. Ayuda a acelerar el entrenamiento y también es extremadamente útil para la visualización de datos.\n",
        "\n",
        "\n",
        "La técnica de reducción de dimensionalidad más popular es el Análisis de Componentes Principales (PCA), que se analiza a continuación.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlw5h1wE9lKA"
      },
      "source": [
        "## Introducción al Análisis de Componentes Principales (PCA)\n",
        "\n",
        "\n",
        "**Análisis de componentes principales (PCA)** es una técnica de reducción de dimensionalidad que se puede utilizar para reducir un conjunto más grande de variables de características a un conjunto más pequeño que aún contiene la mayor parte de la varianza en el conjunto más grande.\n",
        "\n",
        "### Preservar la variación\n",
        "\n",
        "PCA primero identifica el hiperplano más cercano a los datos y luego los proyecta en él. Antes de que podamos proyectar el conjunto de entrenamiento en un hiperplano de dimensiones inferiores, debemos seleccionar el hiperplano correcto. La proyección se puede realizar de tal manera que se preserve la varianza máxima. Esta es la idea detrás de PCA.\n",
        "\n",
        "### Componentes principales\n",
        "\n",
        "PCA identifica los ejes que representan la cantidad máxima de suma acumulada de varianza en el conjunto de entrenamiento. Estos se denominan componentes principales. PCA supone que el conjunto de datos se centra en el origen. Las clases PCA de Scikit-Learn se encargan de centrar los datos automáticamente.\n",
        "\n",
        "### Proyectando hasta d Dimensiones\n",
        "\n",
        "Una vez que hayamos identificado todos los componentes principales, podemos reducir la dimensionalidad del conjunto de datos a d dimensiones proyectándolo en el hiperplano definido por los primeros d componentes principales. Esto garantiza que la proyección conserve la mayor variación posible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZuOC9c39lKB"
      },
      "source": [
        "## Import Python libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "lplIuZYt9lKB"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# import libraries for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVvu197ay1jV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, f1_score, recall_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier, Lasso, ElasticNet\n",
        "from sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV, LassoCV, ElasticNetCV\n",
        "\n",
        "def calculate_metrics(y_true, y_pred_prob, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calcula las métricas AUC, Gini, F1-score y Recall para un modelo de regresión logística.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: array-like, Ground truth (correct) target values.\n",
        "    - y_pred_prob: array-like, Predicted probabilities for the positive class.\n",
        "    - threshold: float, Threshold for converting predicted probabilities to binary predictions.\n",
        "\n",
        "    Returns:\n",
        "    - metrics: dict, Dictionary containing AUC, Gini, F1-score and Recall.\n",
        "    \"\"\"\n",
        "    # Calcular las predicciones binarias basadas en el umbral\n",
        "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
        "\n",
        "    # Calcular AUC\n",
        "    auc = roc_auc_score(y_true, y_pred_prob)\n",
        "\n",
        "    # Calcular Gini\n",
        "    gini = 2 * auc - 1\n",
        "\n",
        "    # Calcular F1-score\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    # Calcular Recall\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "\n",
        "    # Guardar métricas en un diccionario\n",
        "    metrics = {\n",
        "        'AUC': auc,\n",
        "        'Gini': gini,\n",
        "        'F1-score': f1,\n",
        "        'Recall': recall\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def find_best_elasticnet_params(X, y, alphas=np.logspace(-6, 6, 13), l1_ratio=np.linspace(0.01, 1, 10), cv=5):\n",
        "    \"\"\"\n",
        "    Encuentra los mejores parámetros para la regresión logística utilizando ElasticNetCV.\n",
        "\n",
        "    Parameters:\n",
        "    - X: array-like, Características de entrada.\n",
        "    - y: array-like, Etiquetas de clase.\n",
        "    - alphas: array-like, Valores de alfa a probar.\n",
        "    - l1_ratio: array-like, Valores de l1_ratio a probar.\n",
        "    - cv: int, Número de divisiones en la validación cruzada.\n",
        "\n",
        "    Returns:\n",
        "    - best_alpha: float, Mejor valor de alfa.\n",
        "    - best_l1_ratio: float, Mejor valor de l1_ratio.\n",
        "    \"\"\"\n",
        "    # Inicializar el modelo ElasticNetCV\n",
        "    elasticnet_cv = ElasticNetCV(alphas=alphas, l1_ratio=l1_ratio, cv=cv)\n",
        "\n",
        "    # Ajustar el modelo a los datos\n",
        "    elasticnet_cv.fit(X, y)\n",
        "\n",
        "    # Obtener el mejor valor de alfa\n",
        "    best_alpha = elasticnet_cv.alpha_\n",
        "\n",
        "    # Obtener el mejor valor de l1_ratio\n",
        "    best_l1_ratio = elasticnet_cv.l1_ratio_\n",
        "\n",
        "    return best_alpha, best_l1_ratio\n",
        "\n",
        "\n",
        "def train_elasticnet_classifier(X, y, alpha=1.0, l1_ratio=0.5):\n",
        "    \"\"\"\n",
        "    Entrena un modelo de clasificación Elastic Net.\n",
        "\n",
        "    Parameters:\n",
        "    - X: array-like, Características de entrada.\n",
        "    - y: array-like, Etiquetas de clase.\n",
        "    - alpha: float, Parámetro de regularización (mayor valor significa una mayor regularización).\n",
        "    - l1_ratio: float, Proporción de la regularización L1 (0 significa solo L2, 1 significa solo L1).\n",
        "\n",
        "    Returns:\n",
        "    - elasticnet_model: objeto, Modelo de clasificación Elastic Net entrenado.\n",
        "    \"\"\"\n",
        "    elasticnet_model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
        "    elasticnet_model.fit(X, y)\n",
        "    return elasticnet_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO2RScn89lKC"
      },
      "source": [
        "## Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mdRccuT9lKD"
      },
      "outputs": [],
      "source": [
        "file = ('./adult.csv')\n",
        "df = pd.read_csv(file, encoding='latin-1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtRRz8gz9lKD"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxQQPsBi9lKD"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imokIbdu9lKE"
      },
      "source": [
        "Podemos ver que hay 32561 instancias y 15 atributos en el conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_jAuLku9lKE"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvdP0G5K9lKE"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "DArVVTjo9lKE"
      },
      "source": [
        "El resumen del conjunto de datos muestra que no faltan valores. Pero la vista previa muestra que el conjunto de datos contiene valores codificados como `?`. Entonces, codificaré `?` como valores NaN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x-r1B3k9lKE"
      },
      "source": [
        "### Encode `?` as `NaNs`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_W2I2vM9lKE"
      },
      "outputs": [],
      "source": [
        "df[df == '?'] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8Klay6K9lKF"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZiYtqqd9lKF"
      },
      "source": [
        "Ahora, el resumen muestra que las variables `workclass`, `occupation` y `native.country` contienen valores faltantes. Todas estas variables son de tipo de datos categóricos. Entonces, imputaré los valores faltantes con el valor más frecuente: la moda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM7Q7HlR_4Nh"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUAt9rWy9lKF"
      },
      "source": [
        "### Imputar valores faltantes con modo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8VaJ_fe9lKF"
      },
      "outputs": [],
      "source": [
        "for col in ['workclass', 'occupation', 'native.country']:\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj9W63we9lKF"
      },
      "source": [
        "### Verifique nuevamente si faltan valores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLskGE7d9lKF"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wZS7zSz9lKF"
      },
      "source": [
        "Ahora podemos ver que no faltan valores en el conjunto de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErdMHp_S9lKF"
      },
      "source": [
        "### Configuración del vector de características y la variable de destino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EWH5QDc9lKG"
      },
      "outputs": [],
      "source": [
        "X = df.drop(['income'], axis=1)\n",
        "y = df['income'].replace({'<=50K':0,'>50K':1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Azl8AJX9lKG"
      },
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW7FWFSXAJCZ"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKRR8gJI9lKG"
      },
      "source": [
        "## Dividir datos en conjuntos de prueba y entrenamiento separados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6oN45IX9lKG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDwKk41M9lKG"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fxlQ3a69lKG"
      },
      "source": [
        "### Codificar variables categóricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWLJN3Ih9lKG"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "categorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\n",
        "for feature in categorical:\n",
        "        le = preprocessing.LabelEncoder()\n",
        "        X_train[feature] = le.fit_transform(X_train[feature])\n",
        "        X_test[feature] = le.transform(X_test[feature])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpDn1yZz9lKH"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYOmyroz9lKH"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n",
        "X_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqAUYUhb9lKH"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkaOE3E69lKH"
      },
      "source": [
        "## Modelo de regresión logística con todas las características."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfhNfGRQ9lKH"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# print('Puntuación de precisión de regresión logística con todas las características: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
        "\n",
        "# # Evaluar el rendimiento del modelo\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "# class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "# print(\"Precisión del modelo:\", accuracy)\n",
        "# # print(\"Matriz de confusión:\\n\", conf_matrix)\n",
        "# print(\"Informe de clasificación:\\n\", class_report)\n",
        "\n",
        "model_1 = calculate_metrics(y_test, logreg.predict(X_test), threshold=0.5)\n",
        "model_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzRMDOTg9lKI"
      },
      "source": [
        "## Regresión logística con PCA\n",
        "\n",
        "La clase PCA de Scikit-Learn implementa el algoritmo PCA utilizando el siguiente código. Antes de profundizar, explicaré otro concepto importante llamado índice de varianza explicada.\n",
        "\n",
        "\n",
        "### Relación de varianza explicada\n",
        "\n",
        "Un dato muy útil es el **índice de varianza explicada** de cada componente principal. Está disponible a través de la variable `explained_variance_ratio_`. Indica la proporción de la varianza del conjunto de datos que se encuentra a lo largo del eje de cada componente principal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiuZHxih9lKI"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "X_train = pca.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pIBelurGn1p"
      },
      "outputs": [],
      "source": [
        "# Obtener la proporción de varianza explicada por cada componente principal\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Calcular la varianza explicada acumulativa\n",
        "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# Imprimir la proporción de varianza explicada\n",
        "print(\"Proporción de varianza explicada por cada componente principal:\")\n",
        "print(explained_variance_ratio)\n",
        "\n",
        "# Graficar la proporción de varianza explicada y la varianza explicada acumulativa\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Gráfico de barras de la proporción de varianza explicada\n",
        "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.6, align='center', label='Varianza Explicada')\n",
        "\n",
        "# Línea de la varianza explicada acumulativa\n",
        "plt.step(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, where='mid', linestyle='--', label='Varianza Explicada Acumulativa')\n",
        "\n",
        "# Configurar grillas\n",
        "plt.grid(which='both', linestyle='--', linewidth=0.7)\n",
        "plt.gca().yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
        "plt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.1))\n",
        "\n",
        "# Método del codo\n",
        "plt.xlabel('Componentes Principales')\n",
        "plt.ylabel('Proporción de Varianza Explicada')\n",
        "plt.title('Proporción de Varianza Explicada por los Componentes Principales')\n",
        "plt.axhline(y=0.9, color='r', linestyle='-', label='Umbral de 90%')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqe20dmI9lKI"
      },
      "source": [
        "### Comentario\n",
        "\n",
        "- Podemos ver que aproximadamente el 97,25% de la varianza se explica por las 13 primeras variables.\n",
        "\n",
        "- Sólo el 2,75% de la varianza se explica por la última variable. Entonces, podemos suponer que contiene poca información.\n",
        "\n",
        "- Entonces se dejará, y entrenará el modelo nuevamente y con la precisión.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc95i8sAG_LZ"
      },
      "outputs": [],
      "source": [
        "# Cargas factoriales\n",
        "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
        "loadings_df = pd.DataFrame(loadings, columns=[f'PC{i+1}' for i in range(len(loadings))], index=list(X.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHZXvL5ise1U"
      },
      "outputs": [],
      "source": [
        "loadings_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezgYxes-u6n-"
      },
      "outputs": [],
      "source": [
        "# Etiquetas para el eje x\n",
        "etiquetas = X.columns\n",
        "\n",
        "k = 7\n",
        "n_rows = (loadings_df.shape[0] + 1) // k\n",
        "fig, axes = plt.subplots(nrows=n_rows, ncols=k, figsize=(16, 10), sharey=True)\n",
        "\n",
        "# Graficar las componentes principales\n",
        "for i, (nombre_comp, comp) in enumerate(loadings_df.items()):\n",
        "    row, col = i // k, i % k\n",
        "    ax = axes[row, col]\n",
        "    ax.bar(range(len(etiquetas)), comp, color=['teal', 'red'])\n",
        "    ax.set_xticks(range(len(etiquetas)))\n",
        "    ax.set_xticklabels(etiquetas, rotation=90)\n",
        "    ax.axhline(y=0, color='black', linestyle='--')\n",
        "    ax.set_title(nombre_comp)\n",
        "\n",
        "# Ajustar espaciado y eliminar ejes vacíos\n",
        "plt.subplots_adjust(hspace=0.5, wspace=0.3)\n",
        "for ax in axes.flat[loadings_df.shape[0]:]:\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqXMERspLIha"
      },
      "outputs": [],
      "source": [
        "# Ajustar PCA con una cantidad específica de componentes\n",
        "n_components = 12\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca_train = pca.fit_transform(X_train)\n",
        "X_pca_test = pca.fit_transform(X_test)\n",
        "feature_names = X.columns\n",
        "# Proporción de varianza explicada\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# Cargas factoriales\n",
        "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLXC-SYDN0mj"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Entrenar un modelo de regresión logística con los datos transformados\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_pca_train, y_train)\n",
        "\n",
        "model_2 = calculate_metrics(y_test, model.predict(X_pca_test), threshold=0.5)\n",
        "model_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMBErt1HPdAi"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Ajustar PCA con una cantidad específica de componentes\n",
        "n_components = 10\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca_train = pca.fit_transform(X_train)\n",
        "X_pca_test = pca.fit_transform(X_test)\n",
        "\n",
        "# Entrenar un modelo de regresión logística con los datos transformados\n",
        "model3 = LogisticRegression(random_state=42)\n",
        "model3.fit(X_pca_train, y_train)\n",
        "\n",
        "model_3 = calculate_metrics(y_test, model3.predict(X_pca_test), threshold=0.5)\n",
        "model_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpV5D4cq2Uwt"
      },
      "outputs": [],
      "source": [
        "# Ajustar PCA con una cantidad específica de componentes\n",
        "\n",
        "best_alpha, best_l1_ratio = find_best_elasticnet_params(X_train, y_train)\n",
        "logistic_model = train_elasticnet_classifier(X_train, y_train, alpha=best_alpha, l1_ratio=best_l1_ratio)\n",
        "mt_en_1 = calculate_metrics(y_test, logistic_model.predict(X_test), threshold=0.5)\n",
        "mt_en_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skwKCxL3KAnC"
      },
      "outputs": [],
      "source": [
        "# Ajustar PCA con una cantidad específica de componentes\n",
        "n_components = 12\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca_train = pca.fit_transform(X_train)\n",
        "X_pca_test = pca.fit_transform(X_test)\n",
        "\n",
        "best_alpha, best_l1_ratio = find_best_elasticnet_params(X_pca_train, y_train)\n",
        "logistic_model = train_elasticnet_classifier(X_pca_train, y_train, alpha=best_alpha, l1_ratio=best_l1_ratio)\n",
        "mt_en_2 = calculate_metrics(y_test, logistic_model.predict(X_pca_test), threshold=0.5)\n",
        "mt_en_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruavBDRk1zql"
      },
      "outputs": [],
      "source": [
        "# Ajustar PCA con una cantidad específica de componentes\n",
        "n_components = 10\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca_train = pca.fit_transform(X_train)\n",
        "X_pca_test = pca.fit_transform(X_test)\n",
        "\n",
        "best_alpha, best_l1_ratio = find_best_elasticnet_params(X_pca_train, y_train)\n",
        "logistic_model = train_elasticnet_classifier(X_pca_train, y_train, alpha=best_alpha, l1_ratio=best_l1_ratio)\n",
        "mt_en_3 = calculate_metrics(y_test, logistic_model.predict(X_pca_test), threshold=0.5)\n",
        "mt_en_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuE08pZA2aHF"
      },
      "outputs": [],
      "source": [
        "resultados = pd.concat([pd.DataFrame([model_1]),\n",
        "           pd.DataFrame([model_2]),\n",
        "           pd.DataFrame([model_3]),\n",
        "           pd.DataFrame([mt_en_1]),\n",
        "           pd.DataFrame([mt_en_2]),\n",
        "           pd.DataFrame([mt_en_3]),],axis=0,).T\n",
        "\n",
        "resultados.columns = ['Logit_o','Logit_12PC','Logit_10PC','EN_o','EN_12PC','EN_10PC']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TWC_GTz25Em"
      },
      "outputs": [],
      "source": [
        "resultados"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
